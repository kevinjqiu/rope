=============
 Rope Issues
=============


Release Schedule
================

*rope* will be released every two weeks and each quarter a final
version would be released.  That is:

=======  ===============
Week #   Release Number
=======  ===============
2        x.ym1
4        x.ym2
6        x.ym3
8        x.ym4
10       x.ym5
12       x.yrc1
13       x.y
=======  ===============

This is only a rough plan.  For example we might have more release
candidates before the ``x.y`` release.


Version 0.5
===========

From February 4, 2007 till May 6, 2007


Release Goals
-------------

Metaphor: Better static object inference


Discussion
----------

Now that we handle many kinds of refactorings and we have achieved
many of rope's initial goals we can think of extending it.

There are many places to extend rope.  There are two main places
that can be enhanced.  One is occurrence finding mechanisms and the
other is object inference.  Among the features that are going to be
added implicit interfaces and changing structure refactorings are the
most promising ones.

* Occurrence finding
* DOI
* SOI
* Implicit interfaces
* Changing strucutre refactorings; like ``a = b`` -> ``a.set(b)``
* Advanced refactorings


Hot Topics
==========

* `Better SOI`_


To Be Discussed
===============

* `Better occurrence finding`_
* `Allowing non-existent resources`_
* `Python's implicit interfaces`_
* `Having virtual PyModules`_
* `Getting ready for Python 3.0`_
* Indexing source files for faster occurrence finding
* Faster module running
* Saving hard to compute information like class hierarchies to files
* Finding available refactorings


Better SOI
==========

To improve our static object inference mechanisms I propose to add a
new feature to rope to perform enhanced SOI.  Rope analyzes
all function calls in a module and saves the information in
`rope.base.oi.callinfo.CallInforManager` class.

I don't know how often and when to scan a module for
SOI.  Maybe before ``0.5`` release, rope only performs SOI when
when the user askes; just like DOI.  Performing it automatically
requires it to be efficient.  So later, after using it for some
time, we'll decide about that.


Returned Object SOI
-------------------

Since we're passed the objects parameters hold, we can perform a
better returned object SOI.  We can do that by giving parameter
objects passed and infer the object of local names once more.

Actually we have to perform two things.  We have to ask scope
`PyName`\s to forget their concluded data and change the
parameters to return what we want.

So to make it we have:

* Used `PyFunction._set_parameter_pyobjects()` to set the type
  of parameters
* Added `FunctionScope.invalidate_data()` to invalidate the
  pyobjects `AssignedName`\s and `EvaluatedName`\s hold.

And after implementing these we thought of treating function
scopes specially, since the names there cannot be accessed
from outside.


Function Scopes
---------------

Another issue for implementing this feature is that function scope
variable types are dependant on the type of the parameters and
should change.  This issue seem to have some relations with implicit
interfaces.

These two problems(in the above section) points us to treating
function scopes specially.  Since function names are not accessible
from outside that function, we can perform all calculations in a
temporary place.

The problem gets worse when there are defined objects in a function
scope.  Maybe we can change pynames to support function scopes.


Holding Per Name Information
============================

We can change our type DB to add this feature.::

  objectdb.add_per_object(scope, name, function_name, parameters)

* Forcing `create_arguments` to pass the implicit object for
  builtin functions.
* Not ignoring the first argument for builtin types


Better Concluded Data
=====================

The main problem is when using star imports, after a module has been
invalidated its concluded data still exist.  This does not seem to
be a major memory leak, since these `_ConcludedData`\s are invalidated
and contain `None`.

* Using weak references for holding concluded data
* Changing `ImportedName` not to store the imported object in a
  concluded data.  This might slow things a bit.
* Changing `StarImport` to use a mock concluded data for its
  `ImportedModule` and `ImportedName`\s

The other suggestion here is that currently `ImportedName`\s store
object information in the concluded data of the importing module
while they can get concluded data from imported module.  The problem
happens when the imported module gets deleted.

Note that concluded attributes are:

* PyClass: superclass attributes
* PyModule: star imports
* PyPackage: ``__init__.py`` attributes


Marking Errors And Warnings In GUI
==================================

* Marking the occurrence in the buffer
* When to analyze the code?

  * When the user asks
  * When saving

* Saving error/warning information
* ``C-,``, ``C-.``
* ``C-c a p``, ``C-c a n``


Checking Variable And Function Usages In Their Defining Scopes
==============================================================

::

    def a_func(param):
        print param
    a_func('hey')

    class C(object):
        d = {}
        def __init__(self):
            self.d['hey'] = a_func

    d = {}
    d['hey'] = C()


Better Occurrence Finding
=========================

The current implementation for finding occurrences of a `PyName` is to
test every textual occurrence of that name has the same `PyName`
or not.  This approach does not work when a name is repeated many
times in the source.  For example renaming `self`\s are very time
consuming.

Maybe we can use one of these solutions:

* Checking pyname equality only once for each scope

  This way we check each name only once for each scope.  But this is
  not applicable for attributes.  What's more `PyName`\s seem to
  cache their types already.

* Limiting the places to search for a name

  For example for renaming a parameter we search the body of that
  function and the keyword arguments passed to functions.  As
  another example function parameters never appear as attributes
  and class variables and methods never are referenced directly
  except in class body.  The search locations include:

  * module bodies
  * class bodies
  * function bodies

  Access methods include:

  * function call keyword
  * normal name access
  * attribute access

* Excluding unimported modules

  This does not seem to be a good solution.  One reason is that
  the imports might be indirect.  So we have to put a long time
  for creating import trees.

Or maybe we can use a strategy object for searching.


Getting ready for Python 3.0
============================

* Changing print to function in tests
* Not supporting old relative imports
* `iter.__next__()` instead of `iter.next()`
* Supporting `bytes`, `input`
* Removing nonexistent dict methods and methods that will return sets
* Chaning imports after library reorganization
* Show function annotations in pydocs
* ``nonlocal``
* ``except xxx as yyy``
* ``...``
* Supporting keyword only arguments
* Set literals and comprehension ``{1, 2, 3}`` and ``{x for x in range(10)}``
* Renamed function attributes; ``f.func_whatever`` to ``f.__whatever``
* Use new function signature


Allowing Non-Existent Resources
===============================

Instead of doing::

  parent = project.get_resource('my_folder')
  new_file = parent.create_file('my_file.txt')

We can do::

  new_file = project.get_file('my_folder/my_file.txt')
  new_file.create(create_folders=False)


Having Virtual `PyModule`\s
===========================

For doing this we actually need to support `VirtualProject`\s.
Because a refactoring changes the files in a project.  The
`VirtualProject` should compute the changed project tree and
act as a normal `Project`.


What Do We Gain?
----------------

After this refactoring we'll be able to mix any of the refactorings
and move toward bigger refactorings.

* Handling import changes for all modules

  One of the problems we are facing when performing refactorings is
  that imports need to be changed in some of the modules involved in
  that refactoring, but since those modules are already changed for
  that refactoring, it cannot be changed once more easily.

* Performing multiple refactorings in sequence

  For example for performing move method refactoring we can rename the
  self parameter of the method and then move the method itself.  Then
  move the imports used.

* Support for bigger refactorings

  Examples:


Consequences
------------

* Complexity of implementation

  After a change the internal representation of project should be
  updated.  This requires for example changes to `Folder.get_children`
  and `File.read`.

* Inefficiency because of multiple changes while refactoring

  If in a refactoring we perform 3 changes we might also need to
  recompute the information calculated in some of the `PyModule`\s 3
  times.  This seems inefficient.

* Inefficiency due to missing computed information computed locally

  One of the difficulties of having a set of main and many local
  `PyModule`\s is that when we compute some information in local ones
  these information might be no longer valid in global pymodules.


* Not convincing uses

  Right know the only need for performing multiple refactorings is for
  changing imports after performing a refactoring and it has been
  handled using some kind of virtual `PyModule` already.  We could not
  think of any good refactoring that needs to perform multiple basic
  refactorings.

* Lots of changes

  This refactoring needs lots of changes to `PyCore` and modules that
  use it.

* Complex design

  Managing multiple `PyCore`\s and changing `PyObject`\s to work with
  many of them at the same time seems to be hard.


Preventing Unnecessary Recomputations
-------------------------------------

* Using copy on write
* Updating global `PyCore` after performing changes


Using ASTs For Transformations
==============================

The main problem with AST nodes is that they only hold the line in
which statements appear but we need the offset.  If we add offset
information to AST nodes, we would be able to use them for all of the
tasks that we do right now using direct operations on program source
code.

Using ASTs for transforming programs requires making a new AST tree or
altering the old one and writing AST trees.  We can use the latter in
the formatter too but it seems a lot of work.


What Rope Assumes...
====================

In order to simplify problems a bit, rope makes some assumptions about
the source code.  In futures some of this restrictions might be removed.

* All of the modules should use 4 spaces for indenting and no hard tabs.
* All files that end with ``.py`` are considered to be python files and
  all others not.
* Either all files should be under version control or none.
* All ``*.txt`` files are considered in reST formats.
* XXX


Object Inference Issues
=======================


Better Object Inference
-----------------------

Currently rope's object inference mechanisms are simple and there are
many situations that they are of no use:

* A dynamic type inference saves the return types of methods based on
  their parameters
* We use a simple static type inference that:

  * Follows assignments
  * Checks the returned ASTs
  * Uses method return value and parameter information gained by DOI
  * Supports builtin types and containers in a limited way

We are very interested in supporting:

* Implicit interfaces
* Better support for container types based on insertions to them

Possible alternatives:

* Saving the type of local variables when exiting a scope
* Having a set of functions to record the place they are called
* Having a flag that lets us save information per instance
* Saving the result of every call
* Using static type inference algorithms


Rejecting Smalltalk `RefactoringBrowser` Approach
-------------------------------------------------

We can't use smalltalks approach because it requires the whole
tests suite to be run before each refactoring and this does not
seem appealing because it might take a lot of time.

Apart from that if we use a wrapper for a function using python
runtime mechanisms it is possible to find out from which line
a function is called, but we cannot always find the offset in that
file from which this function is called.

Although we cannot find the exact offset an occurrence happens we
can know the suspected lines and that will help us shorten the
scope for searching for occurrences considerably.

If we solve these problems we can can use a strategy for finding
occurrences.  Also Think of other ways of collecting type information.


Saving Collected Data
---------------------

Since resources might change after DTI, we should save collected data
so that they can be used even if there are small changes in some
files.  There might be two approaches to this problem.

In the first approach we can make `PyName`\s and `PyObject`\s
reference objects so that they can be used persistantly.  This
approach seems hard due to problems that might arise for updating
`PyName`\s.

The second approach seems easier to implement.  We can save collected
type information somewhere outside `PyName`\s.  Each time we need a
`PyName`\'s object we use the stored data.  In this approach we
should find someway of storing collected data so it can be used even
if there are some small changes in a resource.  The simplest way of
saving collected data is the ``(file, lineno)`` tuple.  The other is
the hierarchical representation and in this approach we save the data
like ``file.class.method``.


Considerations For A New Textual Form
-------------------------------------

Although textual forms are currently used only for transferring
running data, it can be used for saving `PyObject`\s, too.  This
might be useful if we want to save some of the collected information
to a file.

A textual form should:

* be stored
* have maximum immunity to changes
* represent all kinds of objects

The current textual form:

* ('module', file)
* ('function', file, lineno)
* ('class', file, name)
* ('instance', file, name)
* ('builtin', type, args)
* ('none')
* ('unknown')

Lot's of optimizations is possible.  But they make things more
complicated.  For example By adding a file indexing form:

* ('file', file, index)

We can replace all files in above forms to use the integer indices
specified with file command.  The main problem with this approach is
that file indices might change across different runs.  If transfering
efficiency is really a problem we can add a filter for sending and
receiving that converts the two forms.

We can also remove 'none' and 'unknown' with and empty string.


Returned Types Or Objects
-------------------------

Should we care only about the type of the returned object or we should
consider the object itself, too?

Well, it depends on the type of the objects that is returned.  If it
is an instance of a class then only the type matters.  If it is a
function, module or class then the object is important.

Indeed we should make some difference between the objects that we
only care about their types and the objects that their individual
instance is important to us.  Right now the latter group contains
only functions, classes, modules, lists, dicts and sets but this
list will be probably extended.


Python's Implicit Interfaces
============================

In python you don't have to define interfaces to declare that classes
use a specific protocol or interface.  There are lots of such examples
in standard library.

For example::

  class A(object):

      def a_method(self):
          pass

  class B(object):

      def a_method(self):
          pass

  def a_func(arg):
      arg.a_method()

  a_func(A())
  a_func(B())

Here although there is no inheritance relations between `A` and `B`,
renaming `A.a_method` should force `B.a_method` to be renamed too.

Or as another example::

  a_var = A()
  a_var.a_method()

renaming `a_var.a_method` should rename both `A.a_method` and
`B.a_method`.  That is `a_func` function defines the interface and its
members.

Now the problem is, if rope wants to support implicit interfaces, how
should it find out these interfaces and their members.


Not Supporting Implicit Interfaces
----------------------------------

If we plan not to support implicit interfaces things simplify
considerably.  Function arguments objects are the common base class
of encountered arguments and function returned value object is the
common base class of returned values.


Supporting Implicit Interfaces
------------------------------

We need to find out the way these implicit interfaces are defined.
Actually each function defines an interface for each of its parameters
and the members of this interface are the objects that are passed as
that arguement.  Another thing to keep in mind is that implicit
interfaces are extended by inheritance relation, too.  For example::

  class C(A):

      def a_method(self):
          pass

  def a_func(arg):
      arg.a_method()

  another_func(C())

Renaming `arg.a_method` not only should rename `C.a_method` but also it
it should rename `A.a_method`.

We should be careful about situations like::

  def a_func(arg):
      if isinstance(arg, A):
          arg.do_something()
      else:
          arg.do_some_other_thing()

The other issue is what to do for situations like this::

  def a_func(obj1, obj2):
      result = obj1.f(obj2)
      result.g()

The problem is we don't know the types of `obj1` and `obj2` exactly
and we have a list of types that these variables are assigned during
program execution.  `obj1.f` is not a specific method.


Implementation Issues
---------------------

To support implicit interfaces we need to make lots of changes.  First
of all we know that a function parameter can be anything, so using DOI
we have a set of objects for each parameter.  So the question that
comes up here is when do we need parameter objects.

* When refactoring and finding occurrences
* When code assisting in a function

Also for a rename refactoring we need to check all functions and methods
for implicit interfaces that are involved.  This does not seem practical.
Find someway of making it possible.  Maybe we can find
all of the occurrences of a method and for each check whether it is
invoked on a parameter or not.  Now that we have a bigger set of
methods we need to do the same for these methods.  When we're finished
we can perform the actual refactoring.  The challenge here is that we
should consider inheritance relations, too.

Among `PyName` classes `AssignedName`, `ParameterName` and
`ImportedName` might have more than one object.  And I believe
the main place to change is `StatementEvaluator` that might
return a list of pynames in ``${attribute_accesses}.name``
where `attribute_accessses` might result in more than one kind
of pyname.  This implies that `get_pyname_at` might return a list
of pynames.

`objectinfer` should be changed to find all of the objects that can
be inferred.


The GUI Mess; Working More on the UI Parts
==========================================


Version Control Commands
------------------------

* Commit; ``C-x v c``
* Update; ``C-x v u``
* Diff; ``C-x v d``
* Log; ``C-x v l``
* Revert; ``C-x v r``
* Remove; ``C-x v v``
* Status; ``C-x v s``
* Add; ``C-x v a``


Better Dialogs
--------------

Many dialogs look very similar.  They can be factored.  Continue using
`rope.ui.uihelpers` module and extend it.


Isolating Text Widget Features
------------------------------

Some of the features of GraphicalEditor can be used in other widgets.
Like completion, highlighting.


Defining the Responsibilities of `Core`
---------------------------------------

* Providing an interface to the plug-ins
* Providing methods for controlling rope?
